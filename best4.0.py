# -*- coding: utf-8 -*-
import os
import cv2
import time
import logging
import threading
import subprocess
import queue
import numpy as np
import torch
from ultralytics import YOLO
from tqdm import tqdm

# ==================== âš™ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ ====================

# 1. â±ï¸ è¿è¡Œæ—¶é—´æ§åˆ¶
START_FROM_MINUTES = 15
MAX_PROCESS_MINUTES = 30

# ğŸ”„ [æ ¸å¿ƒè®¾ç½®] ç”»é¢æ—‹è½¬å¼€å…³
ROTATE_VIDEO_180 = False

# 2. ğŸ¯ è‡ªåŠ¨æ ¡å‡†å‚æ•°
CONF_THRES_RIM_INIT = 0.40
CALIBRATION_SAMPLES = 30

# 3. âš¡ï¸ è¿›çƒé€»è¾‘å‚æ•° (ä¼˜åŒ–ç‰ˆ)
HIGH_ZONE_OFFSET = 150
GOAL_ZONE_OFFSET = 150
SHOT_WINDOW = 2.5

# 4. ğŸ¬ å‰ªè¾‘å‚æ•°
CLIP_PRE_TIME = 4.0
CLIP_POST_TIME = 2.0
SHOT_COOLDOWN = 2.0

# 5. ğŸ¤– æ¨¡å‹ä¸è·¯å¾„
MODEL_PATH = "./runs/train/yolo11_finetune_new_court/weights/best.mlpackage"
VIDEO_PATH = "/Users/grifftwu/Desktop/å†å²ç¯®çƒ/1126/111.mp4"
OUTPUT_DIR = "./outputs/auto_mps_clips_1126_rotated1"

# 6. æ¨ç†é…ç½®
CONF_THRES_BALL = 0.15
INFERENCE_SIZE = 1024 

# ========== ğŸš€ æ–°å¢ä¼˜åŒ–é…ç½® ==========
DETECT_WIDTH = 1280     # å°†4Kç”»é¢ç¼©å°åˆ°1280å®½è¿›è¡Œæ£€æµ‹ï¼ˆæå¤§æå‡é€Ÿåº¦ï¼Œä¸å½±å“å‰ªè¾‘ç”»è´¨ï¼‰
FRAME_SKIP = 3          # æ¯éš”2å¸§æ£€æµ‹ä¸€æ¬¡ (ç›¸å½“äº15fpsæ£€æµ‹ï¼Œå®Œå…¨è¶³å¤Ÿæ•æ‰è¿›çƒ)
# ===================================

CLS_BALL = 0
CLS_RIM = 1

# ==================== ç³»ç»Ÿåˆå§‹åŒ– ====================
os.makedirs(OUTPUT_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
logging.getLogger("ultralytics").setLevel(logging.ERROR)

class ClipWorker(threading.Thread):
    """
    å‰ªè¾‘å·¥äººï¼šæ”¯æŒæ—‹è½¬å¯¼å‡º
    """
    def __init__(self, task_queue, rotate_flag):
        super().__init__()
        self.task_queue = task_queue
        self.rotate_flag = rotate_flag
        self.daemon = True 
        self.running = True

    def run(self):
        while self.running:
            try:
                task = self.task_queue.get(timeout=1) 
            except queue.Empty:
                continue
            if task is None: break
            
            source, start, duration, out_path = task
            self.process_video(source, start, duration, out_path)
            self.task_queue.task_done()

    def process_video(self, source, start, duration, out_path):
        try:
            # åŸºç¡€å‘½ä»¤
            cmd = [
                "ffmpeg", "-nostdin", "-y",
                "-ss", f"{start:.3f}",
                "-i", source,
                "-t", f"{duration:.3f}",
                "-loglevel", "error"
            ]

            if self.rotate_flag:
                cmd.extend([
                    "-vf", "transpose=2,transpose=2", 
                    "-c:v", "libx264",        
                    "-preset", "ultrafast",   
                    "-c:a", "copy"            
                ])
            else:
                cmd.extend([
                    "-c", "copy",
                    "-avoid_negative_ts", "1"
                ])

            cmd.append(out_path)
            subprocess.run(cmd, check=True)
            tqdm.write(f"âœ… [å·²ä¿å­˜] {os.path.basename(out_path)}")
        
        except Exception as e:
            logger.error(f"âŒ å‰ªè¾‘å‡ºé”™: {e}")

class AutoMPSDetector:
    def __init__(self, model_path, video_path, start_min, duration_min):
        self.video_path = video_path
        
        self.clip_queue = queue.Queue()
        self.worker = ClipWorker(self.clip_queue, ROTATE_VIDEO_180)
        self.worker.start()
        
        # ç¡¬ä»¶æ£€æŸ¥
        if MODEL_PATH.endswith(".mlpackage"):
            self.device = 'cpu' # CoreML è‡ªåŠ¨æ¥ç®¡
            print(f"âš ï¸ ä½¿ç”¨ CoreML æ¨¡å‹ (Neural Engine åŠ é€Ÿ)")
        elif torch.backends.mps.is_available():
            self.device = 'mps'
            print(f"âš¡ï¸ MPS åŠ é€Ÿå·²å¼€å¯")
        else:
            self.device = 'cpu'

        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        self.model = YOLO(model_path)
        
        # é¢„çƒ­ (ä½¿ç”¨è¾ƒå°çš„å°ºå¯¸é¢„çƒ­)
        dummy = np.zeros((640, 640, 3), dtype=np.uint8)
        self.model.predict(dummy, device=self.device, verbose=False, imgsz=INFERENCE_SIZE)
        
        self.cap = cv2.VideoCapture(video_path)
        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        self.start_frame = int(start_min * 60 * self.fps)
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.start_frame)
        
        if duration_min is None:
            self.end_frame = self.total_frames
        else:
            self.end_frame = min(self.total_frames, self.start_frame + int(duration_min * 60 * self.fps))

        self.is_calibrated = False
        self.calibration_buffer = [] 
        self.locked_hoop_box = None  
        
        self.last_interaction_ts = -10.0
        self.last_shot_ts = -10.0       
        self.shot_count = 0
        
        self.rim_box = []
        self.high_line = 0
        self.goal_zone = []

        # è®¡ç®— Resize æ¯”ä¾‹
        self.original_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.original_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.resize_scale = DETECT_WIDTH / self.original_width
        self.detect_h = int(self.original_height * self.resize_scale)
        self.detect_w = int(DETECT_WIDTH)
        print(f"ğŸ“‰ æ£€æµ‹åˆ†è¾¨ç‡ä¼˜åŒ–: {self.original_width}x{self.original_height} -> {self.detect_w}x{self.detect_h}")

    def run(self):
        print(f"ğŸš€ å¼€å§‹è¿è¡Œ... | è·³å¸§è®¾ç½®: æ¯ {FRAME_SKIP} å¸§æ£€æµ‹ä¸€æ¬¡")
        
        process_len = self.end_frame - self.start_frame
        # 1. ä¿®æ”¹ tqdm é…ç½®ï¼Œå¢åŠ åŠ¨æ€æè¿°åŒºåŸŸ
        pbar = tqdm(total=process_len, unit="frame", ncols=110)
        current_frame_idx = self.start_frame
        
        try:
            while True:
                if current_frame_idx >= self.end_frame: break

                ret, frame = self.cap.read()
                if not ret: break
                
                # è®¡ç®—å½“å‰è§†é¢‘æ—¶é—´ (ç§’)
                current_time = current_frame_idx / self.fps
                
                # 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘å®æ—¶æ›´æ–°è¿›åº¦æ¡å‰ç¼€ï¼Œæ˜¾ç¤º "è§†é¢‘æ—¶é—´ MM:SS"
                # æ¯ 10 å¸§æ›´æ–°ä¸€æ¬¡æ˜¾ç¤ºï¼Œé¿å…åˆ·æ–°è¿‡äºé¢‘ç¹é—ªçƒ
                if current_frame_idx % 10 == 0:
                    mins = int(current_time // 60)
                    secs = int(current_time % 60)
                    # æ˜¾ç¤ºæ ¼å¼ï¼šğŸ” [12:05] ...è¿›åº¦æ¡...
                    pbar.set_description(f"ğŸ” [{mins:02d}:{secs:02d}]")

                # è·³å¸§é€»è¾‘
                if current_frame_idx % FRAME_SKIP != 0 and self.is_calibrated:
                    current_frame_idx += 1
                    pbar.update(1)
                    continue

                # é™é‡‡æ ·
                detect_frame = cv2.resize(frame, (self.detect_w, self.detect_h), interpolation=cv2.INTER_LINEAR)

                if ROTATE_VIDEO_180:
                    detect_frame = cv2.rotate(detect_frame, cv2.ROTATE_180)
                
                if not self.is_calibrated:
                    self._run_calibration(detect_frame)
                else:
                    self._run_inference(detect_frame, current_time)

                pbar.update(1)
                current_frame_idx += 1
                
        except KeyboardInterrupt:
            # 3. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸­æ–­æ—¶è®¡ç®—ä¸‹ä¸€æ¬¡çš„å¯åŠ¨æ—¶é—´
            stop_time = current_frame_idx / self.fps
            stop_min = stop_time / 60.0
            pbar.close() # å…ˆå…³é—­è¿›åº¦æ¡ï¼Œé˜²æ­¢æ‰“å°é”™ä½
            print(f"\n\nğŸ›‘ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢!")
            print(f"ğŸ“Œ å½“å‰è§†é¢‘è¿›åº¦: {int(stop_time//60)}åˆ† {int(stop_time%60)}ç§’")
            # å»ºè®®ç¨å¾®å›é€€ä¸€ç‚¹ç‚¹(æ¯”å¦‚å›é€€0.1åˆ†é’Ÿ)ï¼Œé¿å…æ¼æ‰åˆšå¥½åœ¨è¾¹ç¼˜çš„çƒ
            suggested_start = max(0, stop_min - 0.1)
            print(f"ğŸ‘‰ ä¸‹æ¬¡ç»§ç»­è¿è¡Œè¯·è®¾ç½®: START_FROM_MINUTES = {suggested_start:.2f}\n")
            
        finally:
            if not pbar.disable: # å¦‚æœæ²¡å…³å†å…³ä¸€æ¬¡
                pbar.close()
            self.cap.release()
            self.shutdown()    
  
    def _run_calibration(self, frame):
        # æ³¨æ„ï¼šè¿™é‡Œçš„ frame å·²ç»æ˜¯ç¼©å°åçš„ detect_frame
        results = self.model.predict(
            frame, verbose=False, conf=0.1, iou=0.5, 
            imgsz=INFERENCE_SIZE, classes=[CLS_RIM], device=self.device
        )
        
        if results[0].boxes is not None and len(results[0].boxes) > 0:
            boxes = results[0].boxes
            best_rim = None
            max_conf = 0.0
            
            for box in boxes:
                conf = float(box.conf[0])
                if conf > max_conf:
                    max_conf = conf
                    best_rim = box.xyxy[0].cpu().numpy()
            
            if best_rim is not None and max_conf > CONF_THRES_RIM_INIT:
                self.calibration_buffer.append(best_rim)

        if len(self.calibration_buffer) >= CALIBRATION_SAMPLES:
            self.locked_hoop_box = np.median(self.calibration_buffer, axis=0)
            
            x1, y1, x2, y2 = map(int, self.locked_hoop_box)
            
            # è¿™é‡Œçš„åæ ‡æ˜¯åŸºäº detect_frame (1280å®½) çš„ï¼Œè¿™å®Œå…¨æ²¡é—®é¢˜
            # å› ä¸ºåé¢çš„æ¨ç†ä¹Ÿæ˜¯åŸºäº detect_frame çš„ï¼Œåæ ‡ç³»ä¸€è‡´ã€‚
            self.rim_box = [x1 - 10, y1 - 10, x2 + 10, y2 + 10]
            self.high_line = y1  
            self.goal_zone = [x1 - 30, y1 + 10, x2 + 30, y2 + GOAL_ZONE_OFFSET]
            
            self.is_calibrated = True
            tqdm.write(f"âœ… ç¯®ç­é”å®š (æ£€æµ‹åæ ‡ç³»)! åæ ‡: {self.locked_hoop_box.astype(int)}")

    def _run_inference(self, frame, current_time):
        # ä½¿ç”¨ persist=True å¯ä»¥åœ¨è§†é¢‘æµä¸­ç¨å¾®æå‡ä¸€ç‚¹å†…éƒ¨è¿½è¸ªæ•ˆç‡ï¼Œè™½ç„¶è¿™é‡Œæ²¡ç”¨ tracker
        results = self.model.predict(
            frame, verbose=False, conf=0.01, iou=0.5, 
            imgsz=INFERENCE_SIZE, classes=[CLS_BALL], device=self.device
        )
        self._check_zones_optimized(results, current_time)

    def _check_zones_optimized(self, results, current_time):
        if current_time - self.last_shot_ts < SHOT_COOLDOWN: return

        if results[0].boxes is not None:
            boxes = results[0].boxes
            coords = boxes.xyxy.cpu().numpy()
            confs = boxes.conf.cpu().numpy()
            
            ball_in_goal = False
            
            for i, conf in enumerate(confs):
                if conf > CONF_THRES_BALL:
                    bx1, by1, bx2, by2 = coords[i]
                    cx = (bx1 + bx2) / 2
                    cy = (by1 + by2) / 2
                    
                    is_high = cy < self.high_line
                    
                    rx1, ry1, rx2, ry2 = self.rim_box
                    is_touching_rim = (rx1 < cx < rx2) and (ry1 < cy < ry2)
                    
                    if is_high or is_touching_rim:
                        self.last_interaction_ts = current_time
                        
                    gx1, gy1, gx2, gy2 = self.goal_zone
                    if (gx1 < cx < gx2) and (gy1 < cy < gy2):
                        ball_in_goal = True
            
            if ball_in_goal:
                time_diff = current_time - self.last_interaction_ts
                # SHOT_WINDOW æ˜¯ç§’æ•°ï¼Œä¸å—å¸§ç‡å½±å“ï¼Œæ‰€ä»¥è·³å¸§ä¹Ÿä¸å½±å“è¿™é‡Œçš„é€»è¾‘
                if 0.05 < time_diff < SHOT_WINDOW:
                    self.trigger_goal(current_time)

    def trigger_goal(self, current_time):
        self.shot_count += 1
        self.last_shot_ts = current_time
        
        tqdm.write(f"ğŸ€ [è¿›çƒ!] æ—¶é—´: {current_time:.2f}s | No.{self.shot_count}")
        
        filename = f"goal_{self.shot_count:03d}_{int(current_time)}s.mp4"
        save_path = os.path.join(OUTPUT_DIR, filename)
        
        # å‰ªè¾‘é€»è¾‘å®Œå…¨ä¾èµ–æ—¶é—´æˆ³ï¼Œæ‰€ä»¥ Detect ç¼©æ”¾ä¸å¦ä¸å½±å“å‰ªè¾‘å‡ºçš„è§†é¢‘ç”»è´¨
        start_cut = max(0, current_time - CLIP_PRE_TIME)
        duration = CLIP_PRE_TIME + CLIP_POST_TIME
        
        self.clip_queue.put((self.video_path, start_cut, duration, save_path))

    def shutdown(self):
        print(f"\nğŸ æ‰«æç»“æŸï¼å…±å‘ç°: {self.shot_count} ä¸ªè¿›çƒ")
        if not self.clip_queue.empty():
            print(f"â³ å¤„ç†å‰©ä½™è§†é¢‘ä¸­...")
        self.clip_queue.join()
        self.worker.running = False
        subprocess.run(["open", OUTPUT_DIR])

if __name__ == "__main__":
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ {MODEL_PATH}")
    else:
        detector = AutoMPSDetector(
            MODEL_PATH, 
            VIDEO_PATH, 
            start_min=START_FROM_MINUTES, 
            duration_min=MAX_PROCESS_MINUTES
        )
        detector.run()