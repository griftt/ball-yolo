# -*- coding: utf-8 -*-
import os
import cv2
import time
import logging
import threading
import subprocess
import queue
import numpy as np
import torch
from ultralytics import YOLO
from tqdm import tqdm
import psutil 

# ==================== âš™ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ ====================

# 1. ğŸ“‚ è§†é¢‘ä»»åŠ¡åˆ—è¡¨
VIDEO_TASKS = [
    {"path": "/Users/grifftwu/Desktop/å†å²ç¯®çƒ/1126/111.mp4", "start": 25.25},
    # {"path": "/Users/grifftwu/Desktop/å†å²ç¯®çƒ/1126/222.mp4", "start": 27.97},
    # {"path": "/Users/grifftwu/Desktop/å†å²ç¯®çƒ/1126/333.mp4", "start": 9.51},
    # {"path": "/Users/grifftwu/Desktop/å†å²ç¯®çƒ/1126/444.mp4", "start": 0.5},
    # {"path": "/Users/grifftwu/ball/test2.mp4", "start": 0},
]

# 2. â±ï¸ å…¨å±€é…ç½®
MAX_PROCESS_MINUTES = 30     
OUTPUT_DIR = "./outputs/auto_clips_20260115"

# 3. âš™ï¸ æ€§èƒ½ä¼˜åŒ–é…ç½®
INFERENCE_SIZE = 640        
FRAME_SKIP = 3               # æ¯ 3 å¸§æ£€æµ‹ä¸€æ¬¡
ROTATE_VIDEO_180 = False     

# 4. ğŸ¯ åŠ¨æ€æ£€æµ‹å‚æ•°
CONF_THRES_RIM = 0.03        # ç¯®ç­æ£€æµ‹ç½®ä¿¡åº¦
CONF_THRES_BALL = 0.45       # ç¯®çƒæ£€æµ‹ç½®ä¿¡åº¦
HIGH_ZONE_OFFSET = 150       # é«˜ä½åŒºèŒƒå›´ï¼ˆç¯®ç­ä¸Šæ–¹ï¼‰
GOAL_ZONE_OFFSET = 150       # è¿›çƒåŒºèŒƒå›´ï¼ˆç¯®ç­ä¸‹æ–¹ï¼‰
SHOT_WINDOW = 2.5            # æŠ•ç¯®æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
CLS_BALL = 0
CLS_RIM = 1

# 5. ğŸ¬ å‰ªè¾‘å‚æ•°
CLIP_PRE_TIME = 4.0          
CLIP_POST_TIME = 2.0         
SHOT_COOLDOWN = 3.0          # é˜²æ­¢é‡å¤æ£€æµ‹å†·å´æ—¶é—´

# 6. ğŸ¤– æ¨¡å‹è·¯å¾„
MODEL_PATH = "runs/detect/runs/train/yolo11n_640_train_hd/weights/best.pt"

# ==================== ğŸ›¡ï¸ æ•£çƒ­ä¿æŠ¤é…ç½® ====================
ENABLE_HEAT_PROTECTION = False  
RUN_DURATION_SEC = 600         
REST_DURATION_SEC = 60         

# ==================== ç³»ç»Ÿåˆå§‹åŒ– ====================
os.makedirs(OUTPUT_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
logging.getLogger("ultralytics").setLevel(logging.ERROR)

class ClipWorker(threading.Thread):
    def __init__(self, task_queue, rotate_flag):
        super().__init__()
        self.task_queue = task_queue
        self.rotate_flag = rotate_flag
        self.daemon = True 
        self.running = True

    def run(self):
        while self.running:
            try:
                task = self.task_queue.get(timeout=1) 
            except queue.Empty:
                continue
            if task is None: break
            source, start, duration, out_path = task
            self.process_video(source, start, duration, out_path)
            self.task_queue.task_done()

    def process_video(self, source, start, duration, out_path):
        try:
            cmd = ["ffmpeg", "-nostdin", "-y", "-ss", f"{start:.3f}", "-i", source, "-t", f"{duration:.3f}", "-loglevel", "error"]
            if self.rotate_flag:
                cmd.extend(["-vf", "transpose=2,transpose=2", "-c:v", "libx264", "-preset", "ultrafast", "-c:a", "copy"])
            else:
                cmd.extend(["-c", "copy", "-avoid_negative_ts", "1"])
            cmd.append(out_path)
            subprocess.run(cmd, check=True)
            tqdm.write(f"âœ… [å·²ä¿å­˜] {os.path.basename(out_path)}")
        except Exception as e:
            logger.error(f"âŒ å‰ªè¾‘å‡ºé”™: {e}")

class DynamicDetector:
    """åŠ¨æ€ç¯®ç­æ£€æµ‹ç‰ˆæœ¬ - æ¯å¸§éƒ½å®æ—¶æ£€æµ‹ç¯®ç­ä½ç½®"""
    
    def __init__(self, loaded_model, device, video_path, start_min, duration_min, heat_manager):
        self.model = loaded_model
        self.device = device
        self.video_path = video_path
        self.heat_manager = heat_manager
        
        self.clip_queue = queue.Queue()
        self.worker = ClipWorker(self.clip_queue, ROTATE_VIDEO_180)
        self.worker.start()
        
        # é¢„çƒ­æ¨¡å‹
        dummy = np.zeros((640, 640, 3), dtype=np.uint8)
        self.model.predict(dummy, device=self.device, verbose=False, imgsz=INFERENCE_SIZE)
        
        self.cap = cv2.VideoCapture(video_path)
        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.start_frame = int(start_min * 60 * self.fps)
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.start_frame)
        
        if duration_min is None:
            self.end_frame = self.total_frames
        else:
            self.end_frame = min(self.total_frames, self.start_frame + int(duration_min * 60 * self.fps))

        # åŠ¨æ€æ£€æµ‹çŠ¶æ€
        self.last_interaction_ts = -10.0
        self.last_shot_ts = -10.0       
        self.shot_count = 0

        self.original_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.original_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.resize_scale = INFERENCE_SIZE / self.original_width
        self.detect_w = INFERENCE_SIZE
        self.detect_h = int(self.original_height * self.resize_scale)

    def run(self):
        video_name = os.path.basename(self.video_path)
        print(f"\nğŸ¬ æ­£åœ¨å¤„ç†: {video_name} | è·³å¸§: {FRAME_SKIP} | ğŸš€ åŠ¨æ€æ£€æµ‹æ¨¡å¼: ON")
        
        process_len = self.end_frame - self.start_frame
        pbar = tqdm(total=process_len, unit="frame", ncols=120) 
        current_frame_idx = self.start_frame
        
        current_process = psutil.Process(os.getpid())
        
        try:
            while True:
                if current_frame_idx >= self.end_frame: break

                # ==================== ğŸ›¡ï¸ æ•£çƒ­ä¿æŠ¤ (å…¨å±€) ====================
                if ENABLE_HEAT_PROTECTION:
                    now = time.time()
                    elapsed = now - self.heat_manager['last_rest_time']
                    if elapsed > RUN_DURATION_SEC:
                        pbar.set_description("ğŸ§Š [æ•£çƒ­é™æ¸©ä¸­...]")
                        rest_pbar = tqdm(range(REST_DURATION_SEC), desc="â„ï¸ å€’è®¡æ—¶", leave=False, ncols=80)
                        for _ in rest_pbar: time.sleep(1)
                        
                        self.heat_manager['last_rest_time'] = time.time()
                        pbar.set_description("âš¡ï¸ [æ¢å¤å…¨é€Ÿè¿è¡Œ]")

                # ==================== ğŸš€ è·³å¸§é€»è¾‘ ====================
                if current_frame_idx % FRAME_SKIP != 0:
                    self.cap.grab()
                    current_frame_idx += 1
                    pbar.update(1)
                    continue

                # éœ€è¦æ£€æµ‹çš„å¸§ï¼ŒçœŸæ­£è§£ç 
                ret, frame = self.cap.read()
                if not ret: break
                
                # çŠ¶æ€æ˜¾ç¤º
                if current_frame_idx % 10 == 0:
                    current_time = current_frame_idx / self.fps
                    mins = int(current_time // 60)
                    secs = int(current_time % 60)
                    
                    mem_info = current_process.memory_info()
                    script_mem_gb = mem_info.rss / (1024 ** 3) 
                    sys_mem_percent = psutil.virtual_memory().percent
                    warn_sign = "âš ï¸" if sys_mem_percent > 90 else "ğŸ"
                    
                    desc_str = f"ğŸ” [{mins:02d}:{secs:02d}] | {warn_sign} {script_mem_gb:.1f}G/{sys_mem_percent}%"
                    pbar.set_description(desc_str)

                # é™é‡‡æ ·
                detect_frame = cv2.resize(frame, (self.detect_w, self.detect_h), interpolation=cv2.INTER_LINEAR)
                if ROTATE_VIDEO_180:
                    detect_frame = cv2.rotate(detect_frame, cv2.ROTATE_180)
                
                current_time = current_frame_idx / self.fps
                self._run_dynamic_detection(detect_frame, current_time)

                pbar.update(1)
                current_frame_idx += 1
                
        except KeyboardInterrupt:
            pbar.close()
            stop_time = current_frame_idx / self.fps
            stop_min = stop_time / 60.0
            print(f"\n\nğŸ›‘ [å½“å‰æ–‡ä»¶ä¸­æ–­] {video_name}")
            print(f"ğŸ“Œ ä¸­æ–­æ—¶é—´ç‚¹: {int(stop_time//60)}åˆ† {int(stop_time%60)}ç§’")
            print(f"ğŸ‘‰ è¯¥æ–‡ä»¶æ¢å¤å‚æ•°: \"path\": \"{self.video_path}\", \"start\": {max(0, stop_min - 0.1):.2f}")
            self.shutdown()
            raise KeyboardInterrupt

        finally:
            if not pbar.disable: pbar.close()
            self.cap.release()
            self.shutdown()

    def _run_dynamic_detection(self, frame, current_time):
        """åŠ¨æ€æ£€æµ‹ï¼šæ¯å¸§åŒæ—¶æ£€æµ‹ç¯®çƒå’Œç¯®ç­"""
        
        # åŒæ—¶æ£€æµ‹ç¯®çƒå’Œç¯®ç­
        results = self.model.predict(
            frame, 
            verbose=False, 
            conf=0.1,  # ä½¿ç”¨è¾ƒä½ç½®ä¿¡åº¦ï¼Œåç»­åˆ†åˆ«è¿‡æ»¤
            iou=0.5, 
            imgsz=INFERENCE_SIZE, 
            device=self.device
        )
        
        if results[0].boxes is None or len(results[0].boxes) == 0:
            return
        
        boxes = results[0].boxes
        coords = boxes.xyxy.cpu().numpy()
        confs = boxes.conf.cpu().numpy()
        classes = boxes.cls.cpu().numpy()
        
        # åˆ†ç¦»ç¯®çƒå’Œç¯®ç­
        ball_detections = []
        rim_detections = []
        
        for i, cls in enumerate(classes):
            conf = confs[i]
            box = coords[i]
            
            if cls == CLS_BALL and conf > CONF_THRES_BALL:
                ball_detections.append({
                    'box': box,
                    'conf': conf,
                    'center': ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)
                })
            elif cls == CLS_RIM and conf > CONF_THRES_RIM:
                rim_detections.append({
                    'box': box,
                    'conf': conf,
                    'center': ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)
                })
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç¯®ç­æˆ–ç¯®çƒï¼Œè·³è¿‡
        if not rim_detections or not ball_detections:
            return
        
        # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ç¯®ç­
        best_rim = max(rim_detections, key=lambda x: x['conf'])
        
        # æ£€æŸ¥æ¯ä¸ªç¯®çƒä¸ç¯®ç­çš„å…³ç³»
        self._check_goal_with_dynamic_rim(ball_detections, best_rim, current_time)

    def _check_goal_with_dynamic_rim(self, ball_detections, rim_detection, current_time):
        """ä½¿ç”¨åŠ¨æ€æ£€æµ‹åˆ°çš„ç¯®ç­ä½ç½®åˆ¤æ–­è¿›çƒ"""
        
        # é˜²æ­¢é‡å¤è§¦å‘
        if current_time - self.last_shot_ts < SHOT_COOLDOWN:
            return
        
        rim_box = rim_detection['box']
        rx1, ry1, rx2, ry2 = rim_box
        
        # åŠ¨æ€è®¡ç®—ä¸‰ä¸ªåŒºåŸŸ
        high_line = ry1 - HIGH_ZONE_OFFSET  # é«˜ä½åŒºçº¿
        rim_zone = [rx1 - 10, ry1 - 10, rx2 + 10, ry2 + 10]  # è§¦æ¡†åŒº
        goal_zone = [rx1 - 30, ry1 + 10, rx2 + 30, ry2 + GOAL_ZONE_OFFSET]  # è¿›çƒåŒº
        
        ball_in_goal = False
        
        for ball in ball_detections:
            bx, by = ball['center']
            
            # æ£€æŸ¥é«˜ä½åŒºæˆ–è§¦æ¡†åŒº
            is_high = by < high_line
            is_touching_rim = (rim_zone[0] < bx < rim_zone[2]) and (rim_zone[1] < by < rim_zone[3])
            
            if is_high or is_touching_rim:
                self.last_interaction_ts = current_time
            
            # æ£€æŸ¥è¿›çƒåŒº
            if (goal_zone[0] < bx < goal_zone[2]) and (goal_zone[1] < by < goal_zone[3]):
                ball_in_goal = True
        
        # åˆ¤æ–­æ˜¯å¦è¿›çƒ
        if ball_in_goal:
            time_diff = current_time - self.last_interaction_ts
            if 0.05 < time_diff < SHOT_WINDOW:
                self.trigger_goal(current_time)

    def trigger_goal(self, current_time):
        self.shot_count += 1
        self.last_shot_ts = current_time
        video_base = os.path.splitext(os.path.basename(self.video_path))[0]
        tqdm.write(f"ğŸ€ [è¿›çƒ] {video_base} | æ—¶é—´: {current_time:.2f}s")
        filename = f"{video_base}_goal_{self.shot_count:03d}_{int(current_time)}s.mp4"
        save_path = os.path.join(OUTPUT_DIR, filename)
        start_cut = max(0, current_time - CLIP_PRE_TIME)
        duration = CLIP_PRE_TIME + CLIP_POST_TIME
        self.clip_queue.put((self.video_path, start_cut, duration, save_path))

    def shutdown(self):
        if self.worker.running:
            if not self.clip_queue.empty():
                print(f"â³ æ­£åœ¨å®Œæˆå‰©ä½™å‰ªè¾‘ä»»åŠ¡...")
            self.clip_queue.join()
            self.worker.running = False

# ==================== ä¸»æ§åˆ¶æµç¨‹ ====================
if __name__ == "__main__":
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ {MODEL_PATH}")
        exit()

    print("ğŸ“¦ æ­£åœ¨åŠ è½½ YOLO æ¨¡å‹...")
    device = 'cpu'
    if MODEL_PATH.endswith(".mlpackage"):
        print(f"âš ï¸ ä½¿ç”¨ CoreML æ¨¡å‹ (Neural Engine åŠ é€Ÿ)")
    elif torch.backends.mps.is_available():
        device = 'mps'
        print(f"âš¡ï¸ MPS åŠ é€Ÿå·²å¼€å¯")
    
    loaded_model = YOLO(MODEL_PATH)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œå¼€å§‹å¤„ç†ä»»åŠ¡åˆ—è¡¨...")

    # ğŸ”¥ å…¨å±€æ•£çƒ­ç®¡ç†å™¨
    GLOBAL_HEAT_MANAGER = {'last_rest_time': time.time()}

    try:
        for i, task in enumerate(VIDEO_TASKS):
            path = task["path"]
            start_min = task.get("start", 0.0)
            
            if not os.path.exists(path):
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆè·¯å¾„: {path}")
                continue

            print(f"\n========================================")
            print(f"ğŸ“‚ ä»»åŠ¡ [{i+1}/{len(VIDEO_TASKS)}]: {os.path.basename(path)}")
            print(f"========================================")

            detector = DynamicDetector(
                loaded_model, 
                device, 
                path, 
                start_min, 
                MAX_PROCESS_MINUTES,
                GLOBAL_HEAT_MANAGER
            )
            detector.run()
            
        print("\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼")
        subprocess.run(["open", OUTPUT_DIR])

    except KeyboardInterrupt:
        print("\n\nâ›”ï¸ ----------------------------------------")
        print("â›”ï¸ ç”¨æˆ·å…¨å±€ä¸­æ–­ï¼Œç¨‹åºåœæ­¢ã€‚")
        print("â›”ï¸ ----------------------------------------")
